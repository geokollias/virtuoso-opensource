
This document contains instructions for using the Virtuoso Benchmarks
AMI.  This is a Linux AMI preloaded with the most up to date Virtuoso
Open Source (v7fasttrack) with a selection of TPC-H, LDBC Social Network and LDBC Semantic Publishing  benchmarks at different scales.


This AMI is intended for use with an R3.8xlarge instance.


All necessary programs and modules are preinstalled, and preconfigured
(java, maven, git, hadoop, virtuoso, ldbc_driver, etc), and bash
scripts for all kind of tasks are prepared.

In /home directory, you can find git checkouts from:
- ldbc driver (/home/ldbc_driver)
- ldbc datagenerator (/home/ldbc_snb_datagen)
- virtuoso implmentation of the database connector
  (/home/ldbc_snb_implementations)
- virtuoso v7, feature/analytics branch of v7fasttrack  (/home/virtuoso-opensource)

Also, in the same directory, you have one folder per type of benchmark
and scale factor:
- snb30 - Social Network Benchmark 30GB
- snb300 - Social Network Benchmark 300GB
- snbval - Validation of Social Network Benchmark 
- spb1g - - Semantic Publishing with 1G triples
- spb256 - Semantic Publishing with 256M triples
- spbval - - Validation for Semantic Publishing 
- tpch100 -  100G scale TPC-H
- tpch300 - 300GB scale TPC-H



To get started:

Using your interface of choice, select to launch the xxxx AMI with instance type R3.8xlarge.

Log in with ssh as the ec2-user account.



$ cd ~
$ sudo ./mkvol.sh


This command will make two file systems on the SSD based instance storage of the instance.


SMB 30G and SMB 300G


This section indicates how to  create and load the SNB dataset of
scale factor 30 and 300, and inform you how to run the benchmark
(interactive workload) against it. Here, you can also find the
instructions for SPB and TPC-H.



**********************************
*****           SNB            ***
**********************************


In order to run one of the SNB benchmarks (e.g. SF30), you have to
follow the next steps (if you want to run SF300, everything is the same,
but you have to use different folder, with different scripts - snb300
instead of snb30):

1) Prepare the filesystems, if not already done. Be sure that you have 2 SSDs, 300GB each,
because the total size of the dataset (gzipped) and database files for
SF300 are 138GB and 246GB at the beginning, respectively. For SF30,
these numbers are XXX and XXX. With this configuration, for example,
you can have SF30 and SF300 dataset and database files together, but
you cannot have SNB300 and TPCH300 both in the same time.

$ cd ~
$ sudo ./mkvol.sh


2) Create dataset. Note that this step can take a lot of time (1 hour
for SF30 and more than 7 hours for SF300), but you will avoid it, if
you download the created dataset using the script ./dbget.sh. This option is about 
 60 times faster.

$ cd /home/snb30
$ ./dbgen.sh                  (or: $ ./dbget.sh )


3) Unzip update streams (for SF30, less than 2 minutes, and for SF300
14 minutes)

$ cd /1s1/snb30data/updates/
$ gunzip *.gz


4) Load the dataset (for SF30 this can take about XXX, and for SF300
almost 2 hours). This step will run the Virtuoso server, load it, and
leave it online.

$ cd /home/snb30
$ ./load.sh

During this step (from a second shell), you can check the number of
remaining files to load with the following command:

$ echo 'select count(*) from load_list where ll_state <> 2;' | isql 1210

It will decrease until 0.


5) When the database is loaded, you can check the counts of all the
tables:

$ isql 1210 < chech.sql

If everything is loaded, you should get "OK" as result.


6) Run the benchmark. The default configuration is 48 threads, all
query types enabled (short and long reads and updates), time
compression ratio is XXX for SF30, and 0.7 for SF300. Number of
operations for warmup is XXX for SF30 and 150000 for SF300, while the
numbers of operations in the real run are XXX and 4000000,
respectively. You can expect that warmup and run phase will take about
XXX (SF30) and XXX (SF300) minutes in total. The target throughput is
XXX for SF30 and XXX for SF300.

$ ./run.sh


7) Report the results. The following script can produce the report.

$ ./report.sh > report.txt

The following scripts are not necessery, but can be useful:
- copy the dataset to S3 servers
$ ./dbput.sh
- copy the dataset from S3 servers (SF300 - 11min)
$ ./dbget.sh
- delete generated dataset
$ ./rm_dataset.sh
- delete all database files
$ ./rm_database.sh
- copy database files to S3 servers (SF300 - 39 minutes)
$ ./dbput_db.sh
- copy database files from S3 servers (SF300 - 22 minutes)
$ ./dbget_db.sh

If you want to pass the SNB validation, do the following:
$ cd /home/snbval
$ ./validate.sh




TPC-H

TPC-H is configured at 100G and 300G scales.  cd to /home/tpch100 or /home/tpch300 and do the following:

- Make sure there is space.  If benchmarks were previously run, you should run the rm_database.sh and rm_dataset.sh in the directory of the previous benchmark.

- Generate the data.  Run ./datagen.sh.  For 100G this is single threaded and takes about an hour.  For 300G this is multithreaded and uses a modified dbgen that produces files in gz format.  This takes a few minutes.


- Start the Virtuoso server.
$ ./virtuoso-t +wait 

When the command returns, the database is online.
  
- Load the data 

$ ./load.sh

This takes a about 10 minutes for 100G and 30 minutes for 300G.  During the load, youi can connect to the server and watch the progress.
The server port is 1111.
$ isql 1111
SQL> select ll_file from load_list where ll_state = 1;

shows the set of files loading at the moment in the 300G case.  In the 100G case, all files are loaded at the same time.

In any case one can do select count (*) from lineitem; to see how loading is progressing.

After the load completes, the server is left online and the benchmark can be run.

# ./run.sh


This takes a few minutes for 100G and 3x longer for 300G.  At the end of the run, the files report1.xt and report2.txt appear in the directory.  These contain the standard TPC-H numerical quantities summaries. 

To shutdown the Virtuoso, do 
isql 1111 
SQL> raw_exit ();

or just kill the database process and delete the transaction log.

$ rm /1s2/dbs/virtuoso.trx

If the server is restarted, it will start in the post bulk load state and can do another run.  

**********************************
**** SPB 256M & 1G scale
**********************************

1) In order to run on pre-generated datasets, see /home/spb256/README or /home/spb1g/README
2) In order to run benchmark from scratch do:

 1.1. go to /home/spb256 for 256M scale or /home/spb1g for 1G scale
 1.2. start the Virtuoso server but running ./virtuoso-t 
 1.3. check the virtuoso.log to see when server is up
 1.4. run ./load.sh will take about a hour to complete for 256M scale, and about 5 hours for 1g scale
 1.5. connect with isql on port 1111 or 1112 for 1G scale and check how many triples are loaded by doing "sparql select count(*) where {?s ?p ?o}"
 1.6. run ./run.sh in order to complete the benchmark
 1.7. to stop server execute isql 1111 dba dba exec="raw_exit()", in case of 1G scale SQL port is 1112 and remove /1s2/dbs/spb256.trx or /1s2/dbs/spb1g.trx
 
3) to remove database use rm_database.sh
4) to remove dataset use rm_dataset.sh
