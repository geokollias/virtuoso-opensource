

Virtuoso Benchmark AMI Documentation


This document contains instructions for using the Virtuoso Benchmarks
AMI.  This is a Linux AMI preloaded with the most up to date Virtuoso
Open Source (v7fasttrack, feature/analytics) with a selection of
TPC-H, LDBC Social Network and LDBC Semantic Publishing benchmarks at
different scales.


This AMI is intended for use with an R3.8xlarge instance.


All necessary programs and modules are preinstalled and preconfigured
(java, maven, git, hadoop, virtuoso, ldbc_driver, etc), and bash
scripts for all tasks are predefined.


In the /home directory, you can find git checkouts from:
- ldbc driver (/home/ldbc_driver)
- ldbc datagenerator (/home/ldbc_snb_datagen)
- virtuoso implmentation of the database connector
  (/home/ldbc_snb_implementations)
- virtuoso v7, feature/analytics branch of v7fasttrack  (/home/virtuoso-opensource)

Also, in the same directory, you have one folder per type of benchmark
and scale factor:
- snb100 - Social Network Benchmark 100GB
- snb300 - Social Network Benchmark 300GB
- snbval - Validation of Social Network Benchmark 
- spb1g - - Semantic Publishing with 1G triples
- spb256 - Semantic Publishing with 256M triples
- spbval - - Validation for Semantic Publishing 
- tpch100 -  100G scale TPC-H
- tpch300 - 300GB scale TPC-H

Generally, all benchmarks have scripts for downloading or generating
datasets, bulk loading, running and cleaning up.  Note that the larger
sizes SNB 300 GB and TPC-H 300GB take most of the space on the AWS
instance, so cleaning up is necessary before running the next
benchmark.  The cleanup scripts are called rm_database.sh and
rm_dataset.sh in each directory.



To get started:

Using your interface of choice, select to launch the xxxx AMI with instance type R3.8xlarge.

Log in with ssh as the ec2-user account.



$ cd ~
$ sudo ./mkvol.sh


This command will make two file systems on the SSD based instance storage of the instance, mounted as /1s1 and /1s2.

Many of the procedures described herein require AWS credentials for accessing S3 buckets.

$ aws configure

prompts for the access keys and sets up the AWS command line interface used by the scripts. 

SMB 100G and SMB 300G


Go to the appropriate directory, /home/snb100 for 100G and /home/snb300 for 300G.

1) Prepare the filesystems, if not already done. Be sure that you have 2 SSDs, 300GB each,
because the initial  size of the dataset (gzipped) and database files for
SF300 is 138GB and 246GB, respectively. For SF100,
these numbers are 40G and 75G. 


The benchmark datasets can either be downloaded from S3 or generated from scratch on the instance.  Downloading from S3 is tens of times faster and thus recommended.
The initial state of the database may be created on the instance by bulk loading the dataset or by downloading pre-loaded database files from S3.


2) Create or download dataset. Note that creating the dataset  can take a long time (4 hours
for SF100 and more than 7 hours for SF300).  The dbgen.sh command creates the dataset, the dbget.sh gets it from S3.



$ cd /home/snb100
$ ./dbgen.sh                  (or: $ ./dbget.sh )


3) Unzip update streams (for SF100, less than 8 minutes, and for SF300
14 minutes)

$ cd /1s1/snb100data/updates/
$ gunzip *.gz


4. Bulk Load

You have a choice of downloading pre-loaded database files from S3 or to bulk load the database.  Downloading pre-loaded database files is faster, bulk loading will take a little under 2h for 300GB.  Depending on your preference, do either 4.1 or 4.2.

4.1 Download database files:

$ ./dbget_db.sh 

The command downloads an unzips the database files from S3.

For running, you need to manually start the Virtuoso server:

./virtuoso-t +wait

This returns when the server is online.  Progress can be tracked from the virtuoso.log message log.

4.2) Bulk load the dataset (for SF100 this takes about 40 minutes, and for SF300
almost 2 hours). This step will run the Virtuoso server, load it, and
leave it online.

$ cd /home/snb100
$ ./load.sh

During this step (from a second shell), you can  follow the progress of the load.  Og in with isql:

$ isql 1210
SQL> select count (*) from load_list where ll_state = 0;

The load_list table contains the files to bulk load.  The ll_state
column is 0 for not loaded, 1 for loading and 2 for loaded.  The other
columns are self-explaining.



5) When the database is loaded, you can check the counts of all the
tables:

$ isql 1210 < chech.sql

If everything is loaded, you should get "OK" as result.


6) Run the benchmark. The default configuration is 48 threads, all
query types enabled (short and long reads and updates), time
compression ratio is XXX for SF100, and 1.1 for SF300. Number of
operations for warmup is XXX for SF100 and 150000 for SF300, while the
numbers of operations in the real run are 1500000 and 4000000,
respectively. You can expect that warmup and run phase will take about
XXX (SF100) and XXX (SF300) minutes in total. The target throughput is
XXX for SF100 and XXX for SF300.

$ ./run.sh &

The run.sh script emits continuous output to run.out in the same
directory.  You can follow the progress of the run from this.  The
300G run takes about 2h.  This is due to the scaling rule of SNB that
specifies that a minimum reportable run is 2 hours of simulation time
worth.  The run.sh script or the *.properties files specify a number
of operations.  The number of operations completed is added to the
run.out file every 2 seconds.  The run stops after completing the said number of operations.


7) Report the results. The following script produces the report.

$ ./report.sh > report.txt


To do the recovery test part of LDBC SNB, you may kill the server with
kill -9.  After this, restart it with ./virtuoso-t -f +no-checkpoint &
This will show you the progress of the roll forward recovery on
standard output.  The +no-checkpoint switch causes the log to be left
in place and the contents of the log will not be checkpointed in the
database.  Killing the server after this and deleting the log leaves
the database in the post bulk load state, so that a new run can be made.
The driver cannot be run twice if effects of the previous run are left
in the starting state of the next.

The following scripts are not necessery, but can be useful:
- copy the dataset to S3 servers
$ ./dbput.sh
- copy the dataset from S3 servers (SF300 - 11min)
$ ./dbget.sh
- delete generated dataset
$ ./rm_dataset.sh
- delete all database files
$ ./rm_database.sh
- copy database files to S3 servers (SF300 - 39 minutes)
$ ./dbput_db.sh
- copy database files from S3 servers (SF300 - 22 minutes)
$ ./dbget_db.sh



SNB Validation

To perform the SNB qualification test:

$ cd /home/snbval
$ ./validate.sh




TPC-H


TPC-H is configured at 100G and 300G scales.  cd to /home/tpch100 or
/home/tpch300 and do the following:

- Make sure there is space.  If other benchmarks were previously run, you should run the rm_database.sh and rm_dataset.sh in the directory of the previous benchmark.

- Generate the data.  Run ./datagen.sh.  For 100G this is single threaded and takes about an hour.  For 300G this is multithreaded and uses a modified dbgen that produces files in gz format.  This takes a few minutes.


- Start the Virtuoso server.
$ ./virtuoso-t +wait 

When the command returns, the database is online.
  
- Load the data 

$ ./load.sh

This takes about 10 minutes for 100G and 30 minutes for 300G.
During the load, you can connect to the server and watch the progress.
The server port is 1111.  

$ isql 1111 
SQL> select ll_file from load_list where ll_state = 1;

shows the set of files loading at the moment in the 300G case.  In the 100G case, all files are loaded at the same time.

In any case one can do select count (*) from lineitem; to see how
loading is progressing.  Instead of lineitem you can look at any of
the other tables but lineitem takes the most time.

After the load completes, the server is left online and the benchmark can be run.

# ./run.sh


This takes a few minutes for 100G and 3x longer for 300G.  At the end
of the run, the files report1.xt and report2.txt appear in the
directory.  These contain the standard TPC-H numerical quantities
summaries.  The 100G run is set up with 5 throughput test streams and the 300G with 6.

To shutdown the Virtuoso, do 
isql 1111 
SQL> raw_exit ();

or just kill the database process and delete the transaction log.

$ rm /1s2/dbs/virtuoso.trx

If the server is restarted, it will start in the post bulk load state and can do another run.  

If you stop it with the shutdown command via isql, the effects of the refresh functions will be persisted and the database cannot be reused for another run.


SPB 256M and  1G scale


1) In order to run on pre-generated datasets, see /home/spb256/README or /home/spb1g/README

To run the benchmark from scratch do:

 1.1. go to /home/spb256 for 256M scale or /home/spb1g for 1G scale

The pre-generated datasets are and pre-loaded database files are available on S3. 

***
start the Virtuoso server but running ./virtuoso-t +wait
 1.3. When the command returns the server is online.  You can see the startup messages in virtuoso.log.

In order to avoid loading and data generation which are time consuming we provide a way to use ready SPB database.
Some of commands bellow require access to Amazon S3 storage, for this purpose the 'aws' client must be configured. 

1.1.1) Initialize SSD volumes /1s1 and /1s2
$ sudo ./mkvol.sh 

1.1.2) Download the SPB 256M database backup from the Amzson S3 bucket

$ cd /1s1/
$ aws s3 cp s3://opl-spb256dataset/spb256db.tar .

1.1.3) Decompress the SPB 256M database files, after last command return must have /1s?/dbs/spb256.db files.

$ cd /
$ tar xf /1s1/spb256db.tar 
$ gunzip /1s1/dbs/spb256.db.gz &
$ gunzip /1s2/dbs/spb256.db.gz &
$ wait

1.1.4) Next we remove the backup file to do not take space

$ rm /1s1/spb256db.tar 

1.1.5) Do a copy of the SPB driver on SSD as logs and other files may take a space

$ cd /home/spb256/
$ cp -R /home/ec2-user/ldbc_spb_bm_2.0/dist /1s2/spb256

1.1.6) Do a copy of the the config files for SPB banchmark driver

$ cp *.properties /1s2/spb256

1.1.7) Download the SPB 256M scale datasets and query parameters

$ cd /1s2/spb256/
$ aws s3 cp s3://opl-spb256dataset/spb256data.tar .

1.1.8) Decompress the SPB 256M dataset and query parameters, after that operation the directory /1s2/spb256/generated must be filled with *.nq.gz and query*.txt files.

$ tar xf spb256data.tar

1.1.9) Start the Virtuoso server

$ cd /home/spb256
$ ./virtuoso-t +wait

1.1.10) Check the SPB 256M database contains relevant number of triples, the command bellow should return about 256M triples count

$ isql 1111
SQL> sparql select count(*) { ?s ?p ?o };
SQL> exit;

1.1.11) Run the SPB driver

$ sh run.sh 

1.1.12) Check results from /1s2/spb256/logs

1.1.13) Stop the server, and cleanup the transaction log if need to make second run

$ isql 1111 dba dba exec="raw_exit()"
$ rm /1s?/dbs/spb256.trx


1.2. Running SBP 1Gtriples scale benchmark from pre-loaded database

In order to avoid loading and data generation which are time consuming we provide a way to use ready SPB database.
Some of commands bellow require access to Amazon S3 storage, for this purpose the 'aws' client must be configured. 

1.2.1) Initialize SSD volumes /1s1 and /1s2
$ sudo ./mkvol.sh 

1.2.2) Download the SPB 1Gtriples database backup from the Amzson S3 bucket

$ cd /1s1/
$ aws s3 cp s3://opl-spb1gdataset/spb1gdb.tar .

1.2.3) Decompress the SPB 1Gtriples database files, after last command return must have /1s?/dbs/spb1g.db files.

$ cd /
$ tar xf /1s1/spb1gdb.tar 
$ gunzip /1s1/dbs/spb1g.db.gz &
$ gunzip /1s2/dbs/spb1g.db.gz &
$ wait

1.2.4) Next we remove the backup file to do not take space

$ rm /1s1/spb1gdb.tar 

1.2.5) Do a copy of the SPB driver on SSD as logs and other files may take a space

$ cd /home/spb1g/
$ cp -R /home/ec2-user/ldbc_spb_bm_2.0/dist /1s2/spb1g

1.2.6) Do a copy of the the config files for SPB banchmark driver

$ cp *.properties /1s2/spb1g

1.2.7) Download the SPB 1Gtriples scale datasets and query parameters

$ cd /1s2/spb1g/
$ aws s3 cp s3://opl-spb1gdataset/spb1gdata.tar .
$ aws s3 cp s3://opl-spb1gdataset/spb1gq.tgz .

1.2.8) Decompress the SPB 1Gtriples dataset and query parameters, after that operation the directory /1s2/spb1g/generated must be filled with *.nq.gz and query*.txt files.

$ tar xf spb1gdata.tar
$ tar zxf spb1gq.tgz

1.2.9) Start the Virtuoso server

$ cd /home/spb1g
$ ./virtuoso-t +wait

1.2.10) Check the SPB 1Gtriples database contains relevant number of triples, the command bellow should return about 1Gtriples count

$ isql 1111
SQL> sparql select count(*) { ?s ?p ?o };
SQL> exit;

1.2.11) Run the SPB driver

$ sh run.sh 

1.2.12) Check results from /1s2/spb1g/logs

1.2.13) Stop the server, and cleanup the transaction log if need to make second run

$ isql 1111 dba dba exec="raw_exit()"
$ rm /1s?/dbs/spb1g.trx

2) In order to run SPB from scratch do:

 2.1. go to /home/spb256 for 256M scale or /home/spb1g for 1G scale
 2.2. start the Virtuoso server but running ./virtuoso-t +wait
 2.3. check the virtuoso.log to see when server is up

 2.4. run ./load.sh .  This will take about a hour to complete for 256M scale, and about 5 hours for 1g scale

1) Running benchmarks from pre-generated datasets

1.1.  Running SBP 256M scale benchmark from pre-loaded database

In order to avoid loading and data generation which are time consuming we provide a way to use ready SPB database.
Some of commands bellow require access to Amazon S3 storage, for this purpose the 'aws' client must be configured. 

1.1.1) Initialize SSD volumes /1s1 and /1s2
$ sudo ./mkvol.sh 

1.1.2) Download the SPB 256M database backup from the Amzson S3 bucket

$ cd /1s1/
$ aws s3 cp s3://opl-spb256dataset/spb256db.tar .

1.1.3) Decompress the SPB 256M database files, after last command return must have /1s?/dbs/spb256.db files.

$ cd /
$ tar xf /1s1/spb256db.tar 
$ gunzip /1s1/dbs/spb256.db.gz &
$ gunzip /1s2/dbs/spb256.db.gz &
$ wait

1.1.4) Next we remove the backup file to do not take space

$ rm /1s1/spb256db.tar 

1.1.5) Do a copy of the SPB driver on SSD as logs and other files may take a space

$ cd /home/spb256/
$ cp -R /home/ec2-user/ldbc_spb_bm_2.0/dist /1s2/spb256

1.1.6) Do a copy of the the config files for SPB banchmark driver

$ cp *.properties /1s2/spb256

1.1.7) Download the SPB 256M scale datasets and query parameters

$ cd /1s2/spb256/
$ aws s3 cp s3://opl-spb256dataset/spb256data.tar .

1.1.8) Decompress the SPB 256M dataset and query parameters, after that operation the directory /1s2/spb256/generated must be filled with *.nq.gz and query*.txt files.

$ tar xf spb256data.tar

1.1.9) Start the Virtuoso server

$ cd /home/spb256
$ ./virtuoso-t +wait

1.1.10) Check the SPB 256M database contains relevant number of triples, the command bellow should return about 256M triples count

$ isql 1111
SQL> sparql select count(*) { ?s ?p ?o };
SQL> exit;

1.1.11) Run the SPB driver

$ sh run.sh 

1.1.12) Check results from /1s2/spb256/logs

1.1.13) Stop the server, and cleanup the transaction log if need to make second run

$ isql 1111 dba dba exec="raw_exit()"
$ rm /1s?/dbs/spb256.trx


1.2. Running SBP 1Gtriples scale benchmark from pre-loaded database

In order to avoid loading and data generation which are time consuming we provide a way to use ready SPB database.
Some of commands bellow require access to Amazon S3 storage, for this purpose the 'aws' client must be configured. 

1.2.1) Initialize SSD volumes /1s1 and /1s2
$ sudo ./mkvol.sh 

1.2.2) Download the SPB 1Gtriples database backup from the Amzson S3 bucket

$ cd /1s1/
$ aws s3 cp s3://opl-spb1gdataset/spb1gdb.tar .

1.2.3) Decompress the SPB 1Gtriples database files, after last command return must have /1s?/dbs/spb1g.db files.

$ cd /
$ tar xf /1s1/spb1gdb.tar 
$ gunzip /1s1/dbs/spb1g.db.gz &
$ gunzip /1s2/dbs/spb1g.db.gz &
$ wait

1.2.4) Next we remove the backup file to do not take space

$ rm /1s1/spb1gdb.tar 

1.2.5) Do a copy of the SPB driver on SSD as logs and other files may take a space

$ cd /home/spb1g/
$ cp -R /home/ec2-user/ldbc_spb_bm_2.0/dist /1s2/spb1g

1.2.6) Do a copy of the the config files for SPB banchmark driver

$ cp *.properties /1s2/spb1g

1.2.7) Download the SPB 1Gtriples scale datasets and query parameters

$ cd /1s2/spb1g/
$ aws s3 cp s3://opl-spb1gdataset/spb1gdata.tar .
$ aws s3 cp s3://opl-spb1gdataset/spb1gq.tgz .

1.2.8) Decompress the SPB 1Gtriples dataset and query parameters, after that operation the directory /1s2/spb1g/generated must be filled with *.nq.gz and query*.txt files.

$ tar xf spb1gdata.tar
$ tar zxf spb1gq.tgz

1.2.9) Start the Virtuoso server

$ cd /home/spb1g
$ ./virtuoso-t +wait

1.2.10) Check the SPB 1Gtriples database contains relevant number of triples, the command bellow should return about 1Gtriples count

$ isql 1111
SQL> sparql select count(*) { ?s ?p ?o };
SQL> exit;

1.2.11) Run the SPB driver

$ sh run.sh 

1.2.12) Check results from /1s2/spb1g/logs

1.2.13) Stop the server, and cleanup the transaction log if need to make second run

$ isql 1111 dba dba exec="raw_exit()"
$ rm /1s?/dbs/spb1g.trx

2) In order to run SPB from scratch do:

 2.1. go to /home/spb256 for 256M scale or /home/spb1g for 1G scale
 2.2. start the Virtuoso server but running ./virtuoso-t +wait
 2.3. check the virtuoso.log to see when server is up

 2.4. run ./load.sh .  This will take about a hour to complete for 256M scale, and about 5 hours for 1g scale

 2.5. connect with isql on port 1111 or 1112 for 1G scale and check
 how many triples are loaded by doing 

$isql 1111

SQL>  sparql select count(*) where  {?s ?p ?o};

The triples will be loaded relatively fast. 3/4ths of the time will be spent updating the full text index.  To follow the progress of this, do

SQL> select count (*) from vtlog_db_dba_rdf_obj;

This returns the count of unindexed RDF literals.  When this hits 0 the load is complete.

After the load command returns, the bulk load is complete.  You can do 

SQL> status ();

to see server statistics, for example this should say there are 0 dirty buffers.

 2.6. run ./run.sh to run the benchmark.

The duration is set to 10 minutes of warmup and 20 minutes of running.
 The command produces a running commentary on the progress to standard
 output.  There is a long time of single threaded execution, up to
 several minutes with 1G scale during which the test driver variously
 reads data from the database to control the run.    The The running commentary starts after the warmup period.


 2.7 After the test driver finishes, kill the server.  Use kill -9 or connect with isql and do 

SQL> raw_exit ();

The server port is 1111 for the 256M scale and 1112 for the 1G scale.  Also remember to delete the transaction log.  This is  /1s2/dbs/spb256.trx
 or /1s2/dbs/spb1g.trx

When the server is restarted, it will restart from the post bulk load state.  If modifications made by one run are persisted in the database and a second run is made, results will be off.



3) to delete the database use rm_database.sh
4) to delete the  dataset use rm_dataset.sh


