

Virtuoso Benchmark AMI Documentation


This document contains instructions for using the Virtuoso Benchmarks
AMI.  This is a Linux AMI preloaded with the most up to date Virtuoso
Open Source (v7fasttrack, feature/analytics) with a selection of
TPC-H, LDBC Social Network and LDBC Semantic Publishing benchmarks at
different scales.


This AMI is intended for use with an R3.8xlarge instance.


All necessary programs and modules are preinstalled and preconfigured
(java, maven, git, hadoop, virtuoso, ldbc_driver, etc), and bash
scripts for all tasks are predefined.

In the /home directory, you can find git checkouts from:
- ldbc driver (/home/ldbc_driver)
- ldbc datagenerator (/home/ldbc_snb_datagen)
- virtuoso implmentation of the database connector
  (/home/ldbc_snb_implementations)
- virtuoso v7, feature/analytics branch of v7fasttrack  (/home/virtuoso-opensource)

Also, in the same directory, you have one folder per type of benchmark
and scale factor:
- snb30 - Social Network Benchmark 30GB
- snb300 - Social Network Benchmark 300GB
- snbval - Validation of Social Network Benchmark 
- spb1g - - Semantic Publishing with 1G triples
- spb256 - Semantic Publishing with 256M triples
- spbval - - Validation for Semantic Publishing 
- tpch100 -  100G scale TPC-H
- tpch300 - 300GB scale TPC-H



To get started:

Using your interface of choice, select to launch the xxxx AMI with instance type R3.8xlarge.

Log in with ssh as the ec2-user account.



$ cd ~
$ sudo ./mkvol.sh


This command will make two file systems on the SSD based instance storage of the instance, mounted as /1s1 and /1s2.


SMB 30G and SMB 300G


This section covers loading and running the LDBC SNB benchmarks.



**********************************
*****           SNB            ***
**********************************


In order to run one of the SNB benchmarks (e.g. SF30), you have to
follow the next steps (if you want to run SF300, everything is the same,
but you have to use different folder, with different scripts - snb300
instead of snb30):

1) Prepare the filesystems, if not already done. Be sure that you have 2 SSDs, 300GB each,
because the total size of the dataset (gzipped) and database files for
SF300 are 138GB and 246GB at the beginning, respectively. For SF30,
these numbers are 13G and 25G. With this configuration, for example,
you can have SF30 and SF300 dataset and database files together, but
you cannot have SNB300 and TPCH300 both at the same time.


The benchmark datasets can either be downloaded from S3 or generated from scratch on the instance.  Downloading from S3 is tens of times faster and thus recommended.
The initial state of the database may be created on the instance by bulk loading the dataset or by downloading pre-loaded database files from S3.


2) Create or download dataset. Note that creating the dataset  can take a long time (1 hour
for SF30 and more than 7 hours for SF300).  The dbgen.sh command creates the dataset, the dbget.sh gets it from S3.



$ cd /home/snb30
$ ./dbgen.sh                  (or: $ ./dbget.sh )


3) Unzip update streams (for SF30, less than 2 minutes, and for SF300
14 minutes)

$ cd /1s1/snb30data/updates/
$ gunzip *.gz


4. Bulk Load

You have a choice of downloading pre-loaded database files from S3 or to bulk load the database.  Downloading pre-loaded database files is faster, bulk loading will take a little under 2h for 300GB scale.  Depending on your preference, do either 4.1 or 4.2.

4.1 Download database files:

$ ./dbget_db.sh 

The command downloads an unzips the database files from S3.

For running, you need to manually start the Virtuoso server:

./virtuoso-t +wait

This returns when the server is online.  Progress can be tracked from the virtuoso.log message log.

4.2) Bulk load the dataset (for SF30 this takes about 15 minutes, and for SF300
almost 2 hours). This step will run the Virtuoso server, load it, and
leave it online.

$ cd /home/snb30
$ ./load.sh

During this step (from a second shell), you can  follow the progress of the load.  Og in with isql:

$ isql 1210
SQL> select count (*) from load_list where ll_state = 0;

The load_list table contains the files to bulk load.  The ll_state
column is 0 for not loaded, 1 for loading and 2 for loaded.  The other
columns are self-explaining.



5) When the database is loaded, you can check the counts of all the
tables:

$ isql 1210 < chech.sql

If everything is loaded, you should get "OK" as result.


6) Run the benchmark. The default configuration is 48 threads, all
query types enabled (short and long reads and updates), time
compression ratio is XXX for SF30, and 0.7 for SF300. Number of
operations for warmup is XXX for SF30 and 150000 for SF300, while the
numbers of operations in the real run are XXX and 4000000,
respectively. You can expect that warmup and run phase will take about
XXX (SF30) and XXX (SF300) minutes in total. The target throughput is
XXX for SF30 and XXX for SF300.

$ ./run.sh &

The run.sh script emits continuous output to run.out in the same
directory.  You can follow the progress of the run from this.  The
300G run takes about 2h.  This is due to the scaling rule of SNB that
specifies that a minimum reportable run is 2 hours of simulation time
worth.  The run.sh script or the *.properties files specify a number
of operations.  The number of operations completed is added to the
run.out file every 2 seconds.  The run stops after completing the said number of operations.


7) Report the results. The following script produces the report.

$ ./report.sh > report.txt


To do the recovery test part of LDBC SNB, you may kill the server with
kill -9.  After this, restart it with ./virtuoso-t -f +no-checkpoint &
This will show you the progress of the roll forward recovery on
standard output.  The +no-checkpoint switch causes the log to be left
in place and the contents of the log will not be checkpointed in the
database.  Killing the server after this and deleting the log leaves
the database in post bulk load state, so that a new run can be made.
The driver cannot be run twice if effects of the previous run are left
in the starting state of the next.

The following scripts are not necessery, but can be useful:
- copy the dataset to S3 servers
$ ./dbput.sh
- copy the dataset from S3 servers (SF300 - 11min)
$ ./dbget.sh
- delete generated dataset
$ ./rm_dataset.sh
- delete all database files
$ ./rm_database.sh
- copy database files to S3 servers (SF300 - 39 minutes)
$ ./dbput_db.sh
- copy database files from S3 servers (SF300 - 22 minutes)
$ ./dbget_db.sh



SNB Validation

To perform the SNB qualification test:

$ cd /home/snbval
$ ./validate.sh




TPC-H


TPC-H is configured at 100G and 300G scales.  cd to /home/tpch100 or
/home/tpch300 and do the following:

- Make sure there is space.  If other benchmarks were previously run, you should run the rm_database.sh and rm_dataset.sh in the directory of the previous benchmark.

- Generate the data.  Run ./datagen.sh.  For 100G this is single threaded and takes about an hour.  For 300G this is multithreaded and uses a modified dbgen that produces files in gz format.  This takes a few minutes.


- Start the Virtuoso server.
$ ./virtuoso-t +wait 

When the command returns, the database is online.
  
- Load the data 

$ ./load.sh

This takes a about 10 minutes for 100G and 30 minutes for 300G.
During the load, you can connect to the server and watch the progress.
The server port is 1111.  

$ isql 1111 SQL> select ll_file from
load_list where ll_state = 1;

shows the set of files loading at the moment in the 300G case.  In the 100G case, all files are loaded at the same time.

In any case one can do select count (*) from lineitem; to see how
loading is progressing.  Instead of lineitem you can look at any of
the other tables but lineitem takes the most time.

After the load completes, the server is left online and the benchmark can be run.

# ./run.sh


This takes a few minutes for 100G and 3x longer for 300G.  At the end
of the run, the files report1.xt and report2.txt appear in the
directory.  These contain the standard TPC-H numerical quantities
summaries.

To shutdown the Virtuoso, do 
isql 1111 
SQL> raw_exit ();

or just kill the database process and delete the transaction log.

$ rm /1s2/dbs/virtuoso.trx

If the server is restarted, it will start in the post bulk load state and can do another run.  

If you stop it with the shutdown command via isql, the effects of the refresh functions will be persisted and the database cannot be reused for another run.


SPB 256M & 1G scale


1) In order to run on pre-generated datasets, see /home/spb256/README or /home/spb1g/README
2) In order to run benchmark from scratch do:

 1.1. go to /home/spb256 for 256M scale or /home/spb1g for 1G scale
 1.2. start the Virtuoso server but running ./virtuoso-t +wait
 1.3. check the virtuoso.log to see when server is up

 1.4. run ./load.sh .  This will take about a hour to complete for 256M scale, and about 5 hours for 1g scale

 1.5. connect with isql on port 1111 or 1112 for 1G scale and check
 how many triples are loaded by doing 

$isql 1111

SQL>  sparql select count(*) where  {?s ?p ?o};

The triples will be loaded relatively fast. 3/4ths of the time will be spent updating the full text index.  To follow the progress of this, do

SQL> select count (*) from vtlog_db_dba_rdf_obj;

This returns the count of unindexed RDF literals.  When this hits 0 the load is complete.

After the load command returns, the bulk load is complete.  You can do 

SQL> status ();

to see server statistics, for example this should say there are 0 dirty buffers.

 1.6. run ./run.sh to run the benchmark.

The duration is set to 10 minutes of warmup and 20 minutes of running.
 The command produces a running commentary on the progress to standard
 output.  There is a long time of single threaded execution, up to
 several minutes with 1G scale during which the test driver variously
 reads data from the database to control the run.    The The running commentary starts after the warmup period.


1.7 After the test driver finishes, kill the server.  Use kill -9 or connect with isql and do 

SQL> raw_exit ();

The server port is 1111 for the 256M scale and 1112 for the 1G scale.  Also remember to delete the transaction log.  This is  /1s2/dbs/spb256.trx
 or /1s2/dbs/spb1g.trx

When the server is restarted, it will restart from the post bulk load state.  If modifications made by one run are persisted in the database and a second run is made, results will be off.



3) to delete the database use rm_database.sh
4) to delete the  dataset use rm_dataset.sh


